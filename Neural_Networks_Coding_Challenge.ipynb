{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Networks Coding Challenge.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/sulemc/neural-nets-sc/blob/master/Neural_Networks_Coding_Challenge.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "aD8Yb0P8D04e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural Networks Coding Challenge\n",
        "\n",
        "Objectives:\n",
        "  * Write a simple three layer network\n",
        "  * Compute forward propagation for a new sample in three layer network\n",
        "  * Compute backward propagation in the same network\n",
        "  * Use MLPClassifier to train and test a dataset\n",
        "\n",
        "### Background\n",
        "\n",
        "Other than the MLPClassifier objective, you will be working with this neural net during this coding challenge:\n",
        "\n",
        "![Simple Neural Net](https://www.lucidchart.com/publicSegments/view/a5b0773e-7165-450d-99fc-7089891e099a/image.png)"
      ]
    },
    {
      "metadata": {
        "id": "9JrCk2HEPwoI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1. Write a simple three layer network\n",
        "\n",
        "Create variables to store weights and biases for the above network. Initialize each with $0.5$."
      ]
    },
    {
      "metadata": {
        "id": "5UeD3N5_PwCF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# CODE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jSfpHHfJFPc5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2. Compute forward propagation for a new sample in three layer network\n",
        "\n",
        "Write a function `feed_forward` that takes a new sample $x$ and calculates $\\hat{y}$ via forward propagation."
      ]
    },
    {
      "metadata": {
        "id": "oI260RxPrUV7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x1 = 4\n",
        "x2 = 0.5\n",
        "x3 = 0.125\n",
        "y1 = 0\n",
        "y2 = 1\n",
        "y3 = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x3K306_pFUQc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# CODE\n",
        "\n",
        "\n",
        "# TEST\n",
        "y_hat1 = feed_forward(x1)\n",
        "y_hat2 = feed_forward(x2)\n",
        "y_hat3 = feed_forward(x3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xS6VlmpYTzPx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3. Compute backward propagation for the same network\n",
        "\n",
        "The backprop algorithm is derived from the goal of minimizing the error (or loss) function $\\epsilon = (y - \\hat{y})^2$.\n",
        "\n",
        "$\\epsilon = (y - \\sigma(h_2+b_2))^2$\n",
        "\n",
        "Via the chain rule, the derivative of the above is\n",
        "\n",
        "$\\frac{\\partial \\epsilon}{\\partial \\hat{y}} = 2(y-\\hat{y})\\sigma(h_2)$\n",
        "\n",
        "Let $\\alpha = 0.1$. Update the weights for $h_2$ and $h_1$ via back propagation so that $h_2$ = $h_2 + \\alpha \\frac{\\partial \\epsilon}{\\partial \\hat{y}}$ and $h_1 = h_1 + \\alpha \\frac{\\partial \\epsilon}{\\partial h_2}$\n",
        "\n",
        "Also, let $\\sigma(x) = ReLU(x)$. As such, $\\sigma'(x) = 0$ when $x \\le 0$ and $\\sigma'(x) = 1$ when $x \\gt 0$.\n",
        "\n",
        "Check Case1: of [Brian Dolhansky](http://briandolhansky.com/blog/2013/9/27/artificial-neural-networks-backpropagation-part-4) for a more detailed explanation of the values in the back propagation.\n"
      ]
    },
    {
      "metadata": {
        "id": "Fv_8vZvrT44v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# CODE\n",
        "y_hat4 = feed_forward_and_back_propagate(x1,y1)\n",
        "y_hat5 = feed_forward_and_back_propagate(x2,y2)\n",
        "y_hat6 = feed_forward_and_back_propagate(x3,y3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PYmuA8VwJ4_y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4. Use MLPClassifier to train a dataset\n",
        "\n",
        "`X` is now a small dataset. Create an MLPClassifier from sklearn and train it on the `X` dataset, with `y` as the targets."
      ]
    },
    {
      "metadata": {
        "id": "Jcyi6Z16OGuM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = np.row_stack([x1,x2,x3])\n",
        "y = np.row_stack([y1,y2,y3])\n",
        "\n",
        "# CODE"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}